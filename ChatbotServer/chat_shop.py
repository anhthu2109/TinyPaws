# -*- coding: utf-8 -*-
import os
import time
import faiss
import numpy as np
import pandas as pd
import google.generativeai as genai
import unicodedata
from pymongo import MongoClient, errors
from threading import Thread

# === S·ª¨A L·ªñI ƒê∆Ø·ªúNG D·∫™N ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SHOP_INDEX_PATH = os.path.join(BASE_DIR, "shop_faiss.bin")
SHOP_DATA_PATH = os.path.join(BASE_DIR, "shop_cache.parquet")
# ========================


class ShopRAGMongo:
    def __init__(self, api_key, mongo_uri, db_name="TINYPAWS", collection="products", categories_collection="categories"):
        self.api_key = api_key
        self.mongo_uri = mongo_uri
        self.db_name = db_name
        self.collection_name = collection
        self.categories_collection_name = categories_collection # L∆∞u t√™n b·∫£ng category
        self.embedding_model_name = "models/text-embedding-004"
        
        self.df = pd.DataFrame()
        self.index = None
        self.llm_model = None
        self.db_client = None
        self.db_collection = None
        self.embedding_dimension = 768
        self.similarity_threshold = 0.55 # C√≥ th·ªÉ gi·∫£m xu·ªëng 0.5 n·∫øu mu·ªën t√¨m r·ªông h∆°n

        genai.configure(api_key=self.api_key)
        self.llm_model = genai.GenerativeModel("models/gemini-2.0-flash")
        
        try:
            self.db_client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
            self.db_collection = self.db_client[db_name][collection]
            print(f"K·∫øt n·ªëi MongoDB th√†nh c√¥ng: {db_name}.{collection}")
        except Exception as e:
            print(f"L·ªói k·∫øt n·ªëi MongoDB: {e}")
            self.db_client = None

    # === L·∫•y danh s√°ch Categories v·ªÅ l√†m t·ª´ ƒëi·ªÉn ===
    def get_category_map(self):
        """T·∫°o t·ª´ ƒëi·ªÉn {ID: T√™n Danh M·ª•c}"""
        try:
            cat_coll = self.db_client[self.db_name][self.categories_collection_name]
            # Ch·ªâ l·∫•y _id v√† name ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ
            cursor = cat_coll.find({}, {"_id": 1, "name": 1})
            
            # Map: "68f91e..." -> "Th·ª©c ƒÉn"
            cat_map = {str(doc["_id"]): doc["name"] for doc in cursor}
            return cat_map
        except Exception as e:
            print(f"L·ªói l·∫•y danh m·ª•c: {e}")
            return {}

    # === Load data from MongoDB ===
    def load_data(self):
        if self.db_collection is None:
            return False
            
        try:
            # B∆∞·ªõc 1: L·∫•y t·ª´ ƒëi·ªÉn danh m·ª•c v·ªÅ tr∆∞·ªõc
            cat_map = self.get_category_map()
            print(f"ƒê√£ t·∫£i {len(cat_map)} danh m·ª•c ƒë·ªÉ tham chi·∫øu.")

            # B∆∞·ªõc 2: L·∫•y s·∫£n ph·∫©m (L·∫•y c·∫£ c·ªôt category)
            # L∆∞u √Ω: d√πng stock_quantity theo ƒë√∫ng DB c·ªßa b·∫°n
            projection = {
                "name": 1, "description": 1, "price": 1, 
                "sale_price": 1, "stock_quantity": 1, "category": 1
            }
            products = list(self.db_collection.find({}, projection))
            
            if not products:
                 print("MongoDB r·ªóng.")
                 self.df = pd.DataFrame()
                 return True

            self.df = pd.DataFrame(products)
            self.df["_id"] = self.df["_id"].astype(str)
            if "category" in self.df.columns:
                self.df["category"] = self.df["category"].astype(str)
            
            # B∆∞·ªõc 3: T·∫°o h√†m x·ª≠ l√Ω t·ª´ng d√≤ng ƒë·ªÉ g·∫Øn T√™n Danh M·ª•c v√†o
            def create_full_text(row):
                # X·ª≠ l√Ω gi√°
                price_str = f"{row.get('price', 0)}"
                if row.get('sale_price') and row.get('sale_price') > 0:
                    price_str = f"{row['sale_price']} (G·ªëc: {row['price']})"
                
                # --- QUAN TR·ªåNG: LOOKUP CATEGORY ---
                # L·∫•y ID category t·ª´ s·∫£n ph·∫©m
                cat_id = str(row.get('category', ''))
                # Tra c·ª©u trong t·ª´ ƒëi·ªÉn. N·∫øu kh√¥ng th·∫•y th√¨ ƒë·ªÉ l√† "S·∫£n ph·∫©m"
                cat_name = cat_map.get(cat_id, "S·∫£n ph·∫©m")
                # -----------------------------------

                # Gh√©p chu·ªói th√¥ng minh: ƒê∆∞a T√™n Danh M·ª•c l√™n ƒë·∫ßu
                return (
                    f"Lo·∫°i: {cat_name}. "  # <-- AI s·∫Ω nh√¨n th·∫•y ch·ªØ "Th·ª©c ƒÉn" ·ªü ƒë√¢y
                    f"T√™n: {row['name']}. "
                    f"M√¥ t·∫£: {row.get('description', '')}. "
                    f"Gi√°: {price_str} VND. "
                    f"Kho: {row.get('stock_quantity', 0)}"
                )

            self.df["full_text"] = self.df.apply(create_full_text, axis=1)
            return True

        except Exception as e:
            print(f"L·ªói load data: {e}")
            return False
    
    # === Embedding ===
    def get_embedding(self, text):
        try:
            result = genai.embed_content(model=self.embedding_model_name, content=text)
            return result["embedding"]
        except Exception as e:
            print(f"Error getting embedding: {e}")
            return None

    # === Retry wrapper for LLM ===
    def llm_generate_with_retry(self, prompt, max_retries=3, backoff=2.0):
        for attempt in range(max_retries):
            try:
                response = self.llm_model.generate_content(prompt)
                return response.text
            except Exception as e:
                print(f"L·ªói LLM (l·∫ßn {attempt+1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(backoff * (attempt + 1))
        return "Xin l·ªói, t√¥i t·∫°m th·ªùi kh√¥ng th·ªÉ tr·∫£ l·ªùi l√∫c n√†y."


    # === Build FAISS index (Cosine) ===
    def build_index(self):
        print("ƒêang t·∫°o embeddings cho s·∫£n ph·∫©m...")
        if self.df.empty or 'full_text' not in self.df.columns:
            print("DataFrame r·ªóng, kh√¥ng th·ªÉ build index.")
            self.index = faiss.IndexFlatIP(self.embedding_dimension) 
            return

        self.df["embedding"] = (
            self.df["full_text"].astype(str).apply(self.get_embedding)
        )
        self.df.dropna(subset=["embedding"], inplace=True)

        if self.df.empty:
            print("Kh√¥ng c√≥ embedding n√†o ƒë∆∞·ª£c t·∫°o, index s·∫Ω r·ªóng.")
            self.index = faiss.IndexFlatIP(self.embedding_dimension)
            return

        embeddings = np.array(self.df["embedding"].tolist()).astype("float32")
        faiss.normalize_L2(embeddings)
        self.embedding_dimension = embeddings.shape[1]

        self.index = faiss.IndexFlatIP(self.embedding_dimension)
        self.index.add(embeddings)
        print(f"FAISS index ƒë∆∞·ª£c t·∫°o v·ªõi {len(self.df)} s·∫£n ph·∫©m.")

    # === Cache ===
    def save_cache(self, index_path=SHOP_INDEX_PATH, data_path=SHOP_DATA_PATH):
        try:
            if self.index:
                faiss.write_index(self.index, index_path)
            if not self.df.empty:
                self.df.to_parquet(data_path, index=False, engine='pyarrow')
            print(f"Cache shop ƒë√£ l∆∞u: {index_path}, {data_path}")
        except Exception as e:
            print(f"L·ªói l∆∞u cache: {e}")

    def load_cache(self, index_path=SHOP_INDEX_PATH, data_path=SHOP_DATA_PATH):
        try:
            if os.path.exists(index_path) and os.path.exists(data_path):
                self.index = faiss.read_index(index_path)
                self.df = pd.read_parquet(data_path, engine='pyarrow')
                self.embedding_dimension = self.index.d
                print(f"Cache shop ƒë√£ t·∫£i ({len(self.df)} s·∫£n ph·∫©m).")
                return True
            print("Kh√¥ng t√¨m th·∫•y cache shop, s·∫Ω build l·∫°i t·ª´ MongoDB.")
            return False
        except Exception as e:
            print(f"L·ªói t·∫£i cache shop: {e}")
            return False

    # === Setup ===
    def setup(self, start_watcher=False):
        print("ƒêang kh·ªüi t·∫°o ShopRAG...")
        if self.load_cache():
            print("ShopRAG ƒë√£ t·∫£i t·ª´ cache!")
        else:
            if not self.load_data():
                print("Kh√¥ng th·ªÉ t·∫£i data shop. B·ªè qua build index.")
                self.index = faiss.IndexFlatIP(self.embedding_dimension) 
            else:
                self.build_index()
                self.save_cache()
            
        print("ShopRAG s·∫µn s√†ng!")
        
        if start_watcher and self.db_collection is not None:
            self.start_change_stream_watcher()
    
    # === Retrieval: Hybrid Search (Vector + Keyword) ===
    def find_relevant_products(self, query, k=8):
        # 1. T√¨m ki·∫øm b·∫±ng Vector (C≈©)
        query_emb = self.get_embedding(query)
        vector_results = pd.DataFrame()
        
        if query_emb is not None and self.index and self.index.ntotal > 0:
            q_vec = np.array([query_emb], dtype="float32")
            faiss.normalize_L2(q_vec)
            D, I = self.index.search(q_vec, k)
            vector_results = self.df.iloc[I[0]].copy()
            # G√°n ƒëi·ªÉm gi·∫£ l·∫≠p cho vector search
            vector_results["score"] = D[0]

        # 2. T√¨m ki·∫øm b·∫±ng T·ª´ kh√≥a (M·ªõi - Keyword Search)
        # M·ª•c ƒë√≠ch: B·∫Øt d√≠nh c√°c t·ª´ chuy√™n m√¥n nh∆∞ "s·ªèi th·∫≠n", "tri·ªát s·∫£n", "royal canin"...
        keyword_results = pd.DataFrame()
        if not self.df.empty:
            query_lower = query.lower()
            # T√°ch c√¢u h·ªèi th√†nh c√°c t·ª´ quan tr·ªçng (b·ªè qua c√°c t·ª´ v√¥ nghƒ©a n·∫øu mu·ªën)
            # ·ªû ƒë√¢y ta t√¨m c√°c d√≤ng m√† c·ªôt full_text ch·ª©a c·ª•m t·ª´ ng∆∞·ªùi d√πng h·ªèi
            # V√≠ d·ª•: N·∫øu h·ªèi "s·ªèi th·∫≠n", l·ªçc t·∫•t c·∫£ sp c√≥ ch·ªØ "s·ªèi th·∫≠n"
            
            # ƒê·ªãnh nghƒ©a c√°c t·ª´ kh√≥a "b·∫Øt bu·ªôc ph·∫£i c√≥" n·∫øu xu·∫•t hi·ªán
            important_keywords = ["s·ªèi th·∫≠n", "th·∫≠n", "tri·ªát s·∫£n", "b·∫ßu", "mang thai", "m√®o con", "royal canin", "ganador"]
            
            matched_indices = set()
            for kw in important_keywords:
                if kw in query_lower:
                    # T√¨m c√°c d√≤ng ch·ª©a t·ª´ kh√≥a n√†y
                    matches = self.df[self.df["full_text"].str.contains(kw, case=False, na=False)]
                    if not matches.empty:
                        matched_indices.update(matches.index.tolist())

            if matched_indices:
                keyword_results = self.df.loc[list(matched_indices)].copy()
                keyword_results["score"] = 1.0 # G√°n ƒëi·ªÉm cao nh·∫•t cho k·∫øt qu·∫£ kh·ªõp t·ª´ kh√≥a

        # 3. G·ªôp k·∫øt qu·∫£ (Merge)
        # ∆Øu ti√™n Keyword Search l√™n ƒë·∫ßu, sau ƒë√≥ ƒë·∫øn Vector Search
        final_results = pd.concat([keyword_results, vector_results])
        
        # Lo·∫°i b·ªè tr√πng l·∫∑p (d·ª±a tr√™n _id)
        final_results = final_results.drop_duplicates(subset=["_id"])
        
        # L·∫•y Top K (N·∫øu keyword t√¨m ra √≠t th√¨ b√π b·∫±ng vector, n·∫øu nhi·ªÅu th√¨ l·∫•y h·∫øt keyword tr∆∞·ªõc)
        final_results = final_results.head(k)
        
        if final_results.empty:
             return pd.DataFrame(), []
             
        return final_results, final_results["score"].tolist()

    # === Generation ===
    def generate_answer(self, query, relevant_data):
        if relevant_data.empty:
             return self.llm_generate_with_retry(f"B·∫°n l√† tr·ª£ l√Ω c·ªßa TinyPaws. Hi·ªán kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m n√†o kh·ªõp v·ªõi: '{query}'. H√£y m·ªùi kh√°ch xem c√°c danh m·ª•c kh√°c.")

        # --- X·ª¨ L√ù D·ªÆ LI·ªÜU TR∆Ø·ªöC KHI G·ª¨I AI (QUAN TR·ªåNG) ---
        context_list = []
        for _, row in relevant_data.iterrows():
            # 1. C·∫Øt ng·∫Øn m√¥ t·∫£: Ch·ªâ l·∫•y 200 k√Ω t·ª± ƒë·∫ßu ti√™n ƒë·ªÉ tr√°nh l·ªói 429
            full_desc = str(row.get('description', ''))
            short_desc = full_desc[:200] + "..." if len(full_desc) > 200 else full_desc
            
            # 2. L·∫•y th√¥ng tin lo·∫°i t·ª´ full_text ho·∫∑c c·ªôt category (ƒë√£ map)
            # T√°ch l·∫•y ph·∫ßn "Lo·∫°i: ..." trong full_text ƒë·ªÉ AI d·ªÖ ph√¢n bi·ªát
            full_text_str = str(row.get('full_text', ''))
            category_info = full_text_str.split('.')[0] if "Lo·∫°i:" in full_text_str else f"Lo·∫°i: {row.get('category', 'S·∫£n ph·∫©m')}"

            # 3. T·∫°o chu·ªói th√¥ng tin g·ªçn nh·∫π
            item_str = (
                f"{category_info} | "
                f"T√™n: {row['name']} | "
                f"Gi√°: {row['price']} | "
                f"Kho: {row['stock_quantity']} | "
                f"M√¥ t·∫£: {short_desc}"
            )
            context_list.append(item_str)
        
        context = "\n".join(context_list)
        # ----------------------------------------------------
        
        prompt = f"""
        B·∫°n l√† nh√¢n vi√™n TinyPaws. D∆∞·ªõi ƒë√¢y l√† danh s√°ch s·∫£n ph·∫©m t√¨m ƒë∆∞·ª£c trong kho:
        {context}

        C√¢u h·ªèi c·ªßa kh√°ch: "{query}"

        Nhi·ªám v·ª•:
        1. L·ªåC S·∫¢N PH·∫®M:
           - N·∫øu kh√°ch h·ªèi "M√®o", h√£y ∆ØU TI√äN c√°c s·∫£n ph·∫©m c√≥ ch·ªØ "M√®o" trong T√™n ho·∫∑c Lo·∫°i.
           - N·∫øu kh√°ch h·ªèi "Th·ª©c ƒÉn", ƒê·ª™NG gi·ªõi thi·ªáu B√°t ƒÉn hay D√¢y d·∫Øt (tr·ª´ khi kh√¥ng c√≤n g√¨ kh√°c).
           
        2. TR·∫¢ L·ªúI:
           - Li·ªát k√™ 3 s·∫£n ph·∫©m ph√π h·ª£p nh·∫•t.
           - B√°o gi√° v√† t√¨nh tr·∫°ng kho.
           - Ng·∫Øn g·ªçn, kh√¥ng d√†i d√≤ng.
        """
        
        # Gi·∫£m max_retries xu·ªëng 1 ƒë·ªÉ ƒë·ª° t·ªën th·ªùi gian n·∫øu l·ªói
        return self.llm_generate_with_retry(prompt, max_retries=2)

    # === Chat (c√≥ B·ªô L·ªçc C·ª©ng - Hard Filter) ===
    # === Chat (ƒê√£ th√™m logic Ch√†o h·ªèi & B·ªô l·ªçc c·ª©ng) ===
    def chat(self, query, k=8):
        start = time.time()
        
        # 1. T√¨m ki·∫øm r·ªông (k=8)
        relevant, scores = self.find_relevant_products(query, k)
        
        max_score = 0.0
        if len(scores) > 0:
            max_score = max(scores)

        query_lower = query.lower()

        # --- LOGIC M·ªöI: KI·ªÇM TRA C√ÇU CH√ÄO H·ªéI ---
        # Danh s√°ch c√°c t·ª´ x√£ giao th∆∞·ªùng g·∫∑p
        GREETING_KEYWORDS = ["hi", "hello", "ch√†o", "alo", "∆°i", "shop", "ad", "admin", "b·∫°n ∆°i", "bot", "l√† ai", "gi√∫p"]
        is_greeting = any(kw in query_lower for kw in GREETING_KEYWORDS)
        # ----------------------------------------

        # -------------------------------------------------------
        # B∆Ø·ªöC 2: LOGIC L·ªåC C·ª®NG (QUAN TR·ªåNG NH·∫§T)
        # -------------------------------------------------------
        if not relevant.empty:
            target_category = None

            # ƒê·ªãnh nghƒ©a t·ª´ kh√≥a ph√¢n lo·∫°i
            keyword_rules = {
                "th·ª©c ƒÉn": "Th·ª©c ƒÉn", "ƒë·ªì ƒÉn": "Th·ª©c ƒÉn", "h·∫°t": "Th·ª©c ƒÉn", "pate": "Th·ª©c ƒÉn", "b√°nh th∆∞·ªüng": "Th·ª©c ƒÉn",
                "ƒë·ªì ch∆°i": "ƒê·ªì ch∆°i", "th√∫ b√¥ng": "ƒê·ªì ch∆°i", "b√≥ng": "ƒê·ªì ch∆°i",
                "ph·ª• ki·ªán": "Ph·ª• ki·ªán", "b√°t": "Ph·ª• ki·ªán", "d√¢y d·∫Øt": "Ph·ª• ki·ªán", "v√≤ng c·ªï": "Ph·ª• ki·ªán", "t√∫i": "Ph·ª• ki·ªán",
                "v·ªá sinh": "V·ªá sinh", "t·∫Øm": "V·ªá sinh", "c√°t": "V·ªá sinh"
            }

            for kw, cat_name in keyword_rules.items():
                if kw in query_lower:
                    target_category = cat_name
                    break 

            if target_category:
                print(f"--> Ph√°t hi·ªán nhu c·∫ßu: {target_category}. ƒêang l·ªçc d·ªØ li·ªáu...")
                filtered_relevant = relevant[relevant["full_text"].str.contains(f"Lo·∫°i: {target_category}", case=False, na=False)]
                
                if not filtered_relevant.empty:
                    relevant = filtered_relevant
                    print(f"--> ƒê√£ l·ªçc c√≤n {len(relevant)} s·∫£n ph·∫©m ƒë√∫ng lo·∫°i.")
                else:
                    print("--> L·ªçc xong kh√¥ng c√≤n g√¨, quay v·ªÅ d√πng danh s√°ch g·ªëc.")
        # -------------------------------------------------------

        print(f"Max similarity (shop) = {max_score:.3f}")

        # === QUY·∫æT ƒê·ªäNH TR·∫¢ L·ªúI ===
        # Tr∆∞·ªùng h·ª£p 1: Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m V√Ä ƒëi·ªÉm th·∫•p
        if relevant.empty or max_score < self.similarity_threshold:
            
            # N·∫æU L√Ä C√ÇU CH√ÄO H·ªéI -> V·∫´n tr·∫£ l·ªùi (Bypass ng∆∞·ª°ng ƒëi·ªÉm)
            if is_greeting:
                print("--> Ph√°t hi·ªán c√¢u ch√†o h·ªèi. Tr·∫£ l·ªùi x√£ giao.")
                greeting_prompt = f"""
                Ng∆∞·ªùi d√πng n√≥i: "{query}"
                B·∫°n l√† tr·ª£ l√Ω ·∫£o c·ªßa TinyPaws. H√£y ch√†o l·∫°i kh√°ch h√†ng m·ªôt c√°ch th√¢n thi·ªán, d·ªÖ th∆∞∆°ng (d√πng icon üêæ, üê±).
                Gi·ªõi thi·ªáu ng·∫Øn g·ªçn b·∫°n c√≥ th·ªÉ gi√∫p h·ªç t√¨m th·ª©c ƒÉn, ph·ª• ki·ªán, ho·∫∑c ƒë·ªì ch∆°i cho th√∫ c∆∞ng.
                """
                return {
                    "response": self.llm_generate_with_retry(greeting_prompt), # G·ªçi AI tr·∫£ l·ªùi ch√†o
                    "sources": [],
                    "processing_time": round(time.time() - start, 2),
                    "max_similarity": float(max_score)
                }
            
            # N·∫æU KH√îNG PH·∫¢I CH√ÄO -> B√°o l·ªói kh√¥ng t√¨m th·∫•y
            else:
                answer = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p. B·∫°n th·ª≠ h·ªèi c·ª• th·ªÉ h∆°n v·ªÅ th·ª©c ƒÉn, ƒë·ªì ch∆°i hay ph·ª• ki·ªán nh√©?"
                docs = []
        
        # Tr∆∞·ªùng h·ª£p 2: T√¨m th·∫•y s·∫£n ph·∫©m (ƒêi·ªÉm cao)
        else:
            # G·ªçi h√†m generate_answer b√¨nh th∆∞·ªùng
            answer = self.generate_answer(query, relevant)
            docs = relevant[["name", "description", "price", "stock_quantity"]].replace({np.nan: None}).to_dict("records")

        return {
            "response": answer,
            "sources": docs,
            "processing_time": round(time.time() - start, 2),
            "max_similarity": float(max_score)
        }
        
    # === Real-time watcher ===
    def reload_index(self):
        """H√†m n√†y ƒë∆∞·ª£c g·ªçi khi c√≥ thay ƒë·ªïi trong DB"""
        print("Ph√°t hi·ªán thay ƒë·ªïi MongoDB! ƒêang build l·∫°i index...")
        if self.load_data():
            self.build_index()
            self.save_cache()
            print("Index shop ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t.")
        
    def start_change_stream_watcher(self):
        print("Theo d√µi thay ƒë·ªïi MongoDB (auto reload)...")
        # === S·ª¨A L·ªñI "is not None" ===
        if self.db_collection is None:
             print("Kh√¥ng th·ªÉ theo d√µi, ch∆∞a k·∫øt n·ªëi MongoDB.")
             return

        try:
            # Ki·ªÉm tra xem Change Streams c√≥ ƒë∆∞·ª£c h·ªó tr·ª£ kh√¥ng
            self.db_client.admin.command('hello')
            print("Change Streams ƒë∆∞·ª£c h·ªó tr·ª£.")
        except Exception as e:
            print(f"Change Streams kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£ (ch·ªâ c√≥ tr√™n cluster M0+): {e}. T·∫Øt auto-reload.")
            return

        def watch_changes():
            try:
                with self.db_collection.watch(full_document='updateLookup') as stream:
                    for change in stream:
                        print(f"MongoDB change detected: {change['operationType']}")
                        # ƒê∆°n gi·∫£n l√† build l·∫°i m·ªçi th·ª© khi c√≥ thay ƒë·ªïi
                        if change['operationType'] in ['insert', 'update', 'replace', 'delete']:
                            self.reload_index()
            except Exception as e:
                print(f"L·ªói Change Stream watcher: {e}")

        # Ch·∫°y watcher trong m·ªôt thread ri√™ng
        watcher_thread = Thread(target=watch_changes, daemon=True)
        watcher_thread.start()
        print("Watcher thread started (Change Stream supported).")