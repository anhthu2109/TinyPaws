# -*- coding: utf-8 -*-
import os
import time
import faiss
import numpy as np
import pandas as pd
import google.generativeai as genai
import unicodedata

# === S·ª¨A L·ªñI ƒê∆Ø·ªúNG D·∫™N ===
# L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa th∆∞ m·ª•c ch·ª©a file chat_rag.py n√†y
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n cache d·ª±a tr√™n BASE_DIR
INDEX_PATH = os.path.join(BASE_DIR, "faiss_index.bin")
DATA_PATH = os.path.join(BASE_DIR, "qa_cache.parquet")
# ========================

class PetChatRAG:
    def __init__(self, api_key, data_file):
        self.api_key = api_key
        self.data_file = data_file
        self.embedding_model_name = "models/text-embedding-004"
        self.df = None
        self.index = None
        self.llm_model = None
        self.embedding_dimension = None
        self.similarity_threshold = 0.55

        genai.configure(api_key=self.api_key)
        self.llm_model = genai.GenerativeModel("models/gemini-2.0-flash")

    # === Load data ===
    def load_data(self):
        try:
            self.df = pd.read_excel(self.data_file)
            print(f"Data loaded from {self.data_file} ({len(self.df)} records)")

            self.df["question"] = (
                self.df["question"]
                .astype(str)
                .str.lower()
                .apply(lambda x: unicodedata.normalize("NFKD", x))
                .str.encode("ascii", errors="ignore")
                .str.decode("utf-8")
            )

            return True
        except Exception as e:
            print(f"Error loading data: {e}")
            return False

    # === Embedding ===
    def get_embedding(self, text):
        try:
            result = genai.embed_content(model=self.embedding_model_name, content=text)
            return result["embedding"]
        except Exception as e:
            print(f"Error getting embedding: {e}")
            return None

    # === Retry wrapper for LLM ===
    def llm_generate_with_retry(self, prompt, max_retries=3, backoff=2.0):
        for attempt in range(max_retries):
            try:
                response = self.llm_model.generate_content(prompt)
                return response.text
            except Exception as e:
                print(f"L·ªói LLM (l·∫ßn {attempt+1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(backoff * (attempt + 1))
        return "Xin l·ªói, t√¥i t·∫°m th·ªùi kh√¥ng th·ªÉ tr·∫£ l·ªùi l√∫c n√†y."

    # === Build FAISS index (Cosine) ===
    def build_index(self):
        print("Building embeddings...")
        self.df["embedding"] = (
            self.df.apply(
                lambda x: self.get_embedding(f"{x['question']} {x['answers']}"),
                axis=1
            )
        )
        self.df.dropna(subset=["embedding"], inplace=True)

        embeddings = np.array(self.df["embedding"].tolist()).astype("float32")
        faiss.normalize_L2(embeddings)
        self.embedding_dimension = embeddings.shape[1]

        self.index = faiss.IndexFlatIP(self.embedding_dimension)
        self.index.add(embeddings)
        print(f"FAISS index built successfully ({len(embeddings)} vectors).")


    # === Cache ===
    def save_cache(self, index_path="faiss_index.bin", data_path="qa_cache.parquet"):
        try:
            faiss.write_index(self.index, index_path)
            self.df.to_parquet(data_path, index=False)
            print(f"Cache saved: {index_path}, {data_path}")
        except Exception as e:
            print(f"Error saving cache: {e}")

    def load_cache(self, index_path="faiss_index.bin", data_path="qa_cache.parquet"):
        try:
            if os.path.exists(index_path) and os.path.exists(data_path):
                self.index = faiss.read_index(index_path)
                self.df = pd.read_parquet(data_path)
                print(f"Cache loaded ({len(self.df)} records).")
                return True
            return False
        except Exception as e:
            print(f"Error loading cache: {e}")
            return False

    # === Setup ===
    def setup_with_cache(self):
        print("Initializing chatbot...")
        if self.load_cache():
            print("Loaded from cache!")
            return
        if not self.load_data():
            raise Exception("Failed to load data file.")
        self.build_index()
        self.save_cache()
        print("Chatbot ready with new embeddings!")

    # === Retrieval ===
    def find_relevant_answers(self, query, k=3):
        query_emb = self.get_embedding(query)
        if query_emb is None:
            return pd.DataFrame(), []

        q_vec = np.array([query_emb], dtype="float32")
        faiss.normalize_L2(q_vec)
        D, I = self.index.search(q_vec, k)
        return self.df.iloc[I[0]], D[0]

    # === Generation ===
    def generate_answer(self, query, relevant_data):
        context = "\n".join(relevant_data["answers"].tolist())
        prompt = f"""
        B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ Th√∫ C∆∞ng (TinyPaws).
        
        Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin tham kh·∫£o.
        
        QUY T·∫ÆC AN TO√ÄN (QUAN TR·ªåNG):
        1. KI·ªÇM TRA ƒê·ªêI T∆Ø·ª¢NG: 
           - N·∫øu c√¢u h·ªèi d√πng ch·ªß ng·ªØ l√† con ng∆∞·ªùi (v√≠ d·ª•: "t√¥i b·ªã...", "ch√¢n t√¥i", "con t√¥i", "ng∆∞·ªùi y√™u"...), h√£y T·ª™ CH·ªêI TR·∫¢ L·ªúI NGAY.
           - Ch·ªâ n√≥i ng·∫Øn g·ªçn: "TinyPaws ch·ªâ chuy√™n t∆∞ v·∫•n s·ª©c kh·ªèe cho ch√≥ m√®o th√¥i ·∫°, sen ƒëi kh√°m b√°c sƒ© ng∆∞·ªùi nha! üêæ".
           - TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞a ra l·ªùi khuy√™n y t·∫ø cho ng∆∞·ªùi (k·ªÉ c·∫£ khi b·∫°n bi·∫øt).
           
        2. CH·ªà TR·∫¢ L·ªúI KHI: C√¢u h·ªèi li√™n quan ƒë·∫øn ch√≥, m√®o, th√∫ c∆∞ng.
        
        Th√¥ng tin tham kh·∫£o (D√†nh cho th√∫ c∆∞ng):
        {context}

        C√¢u h·ªèi: {query}
        """
        return self.llm_generate_with_retry(prompt)

    # === Chat (ƒê√£ s·ª≠a ƒë·ªÉ nh·∫≠n di·ªán Ch√†o h·ªèi x√£ giao) ===
    def chat(self, query, k=3):
        start = time.time()
        
        # T√¨m ki·∫øm d·ªØ li·ªáu li√™n quan
        relevant, scores = self.find_relevant_answers(query, k)

        max_sim = max(scores) if len(scores) else 0.0
        print(f"Max similarity = {max_sim:.3f} (threshold = {self.similarity_threshold})")

        query_lower = query.lower()

        # 1. T·ª´ kh√≥a chuy√™n m√¥n (Gi·ªØ nguy√™n)
        PET_KEYWORDS = ["ch√≥", "cho", "c√∫n", "m√®o", "meo", "pet", "th√∫ c∆∞ng",
                        "r·ªëi lo·∫°n", "b·ªánh", "chƒÉm s√≥c", "ƒÉn", "th·ª©c ƒÉn", "kh·∫©u ph·∫ßn",
                        "t·∫Øm", "spa", "s·ª©c kh·ªèe", "hu·∫•n luy·ªán", "kh√°m", "ch√≥ con"]
        is_pet_query = any(kw in query_lower for kw in PET_KEYWORDS)

        # 2. TH√äM M·ªöI: T·ª´ kh√≥a ch√†o h·ªèi / X√£ giao
        GREETING_KEYWORDS = ["hi", "hello", "ch√†o", "alo", "∆°i", "shop", "ad", "admin", "bot", "gi√∫p", "h√∫", "b·∫°n ∆°i"]
        is_greeting = any(kw in query_lower for kw in GREETING_KEYWORDS)

        # 3. LOGIC CH·∫∂N (S·ª≠a l·∫°i ƒëi·ªÅu ki·ªán l·ªçc)
        # Ch·∫∑n n·∫øu: (Kh√¥ng ph·∫£i t·ª´ kh√≥a Pet V√Ä Kh√¥ng ph·∫£i ch√†o h·ªèi)
        # HO·∫∂C: (ƒêi·ªÉm similarity th·∫•p V√Ä Kh√¥ng ph·∫£i ch√†o h·ªèi)
        if (not is_pet_query and not is_greeting) or (max_sim < self.similarity_threshold and not is_greeting):
            return {
                "response": "TinyPaws ch·ªâ h·ªó tr·ª£ c√°c v·∫•n ƒë·ªÅ v·ªÅ th√∫ c∆∞ng. "
                            "B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ chƒÉm s√≥c ch√≥ m√®o nh√©!",
                "similar_documents": [],
                "processing_time": round(time.time() - start, 2),
                "max_similarity": round(max_sim, 3)
            }

        # 4. X·ª¨ L√ù TR·∫¢ L·ªúI
        # Tr∆∞·ªùng h·ª£p A: Ch·ªâ l√† c√¢u ch√†o h·ªèi x√£ giao (ƒêi·ªÉm th·∫•p, kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu y t·∫ø)
        if is_greeting and max_sim < self.similarity_threshold:
            prompt = f"""
            Ng∆∞·ªùi d√πng n√≥i: "{query}"
            B·∫°n l√† chuy√™n gia chƒÉm s√≥c th√∫ c∆∞ng (AI) c·ªßa TinyPaws.
            H√£y ch√†o l·∫°i ng∆∞·ªùi d√πng m·ªôt c√°ch th√¢n thi·ªán, ng·∫Øn g·ªçn, d√πng emoji üêæ.
            G·ª£i √Ω h·ªç c√≥ th·ªÉ h·ªèi v·ªÅ: s·ª©c kh·ªèe, dinh d∆∞·ª°ng, ho·∫∑c c√°ch hu·∫•n luy·ªán ch√≥ m√®o.
            """
            answer = self.llm_generate_with_retry(prompt)
            docs = []

        # Tr∆∞·ªùng h·ª£p B: C√≥ n·ªôi dung chuy√™n m√¥n (ƒêi·ªÉm cao ho·∫∑c c√≥ t·ª´ kh√≥a Pet)
        else:
            # D√πng h√†m generate_answer c√≥ s·∫µn ƒë·ªÉ tr·∫£ l·ªùi d·ª±a tr√™n Knowledge Base
            answer = self.generate_answer(query, relevant)
            docs = relevant[["question", "answers"]].replace({np.nan: None}).to_dict("records")

        return {
            "response": answer,
            "similar_documents": docs,
            "processing_time": round(time.time() - start, 2),
            "max_similarity": round(max_sim, 3)
        }